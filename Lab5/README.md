# Boutique App Istio Fault Injection and Monitoring Guide

This documents the process of LAB 5 (monitoring services within the Boutique app), focusing on request rates, fault injection, and handling duplicate entries in Prometheus. Each section includes explanations and solutions to common challenges encountered, like duplicate data handling. Screenshots for each task are organized in folders `task1`, `task2`, and `task3`.

## Task 1: Monitoring Request Rates

### Step 1: Querying the Request Rate for a Service

To observe the rate of requests entering a specific service in the Boutique app, we use the `istio_requests_total` metric and focus on a one-minute interval.

### Prometheus Query for Request Rate

Run the following query to see the rate of requests:

```prometheus
rate(istio_requests_total{destination_canonical_service="<service_name>"}[1m])
```

This query:

- **Filters requests** by the destination service using `destination_canonical_service`
- **Calculates the rate** over a one-minute window

### Duplicate Results Explanation

There are duplicate entries observed in the Prometheus results, caused by:

1. **Multiple instances or replicas** of the service
2. **Different source services** sending requests to the destination service, each tracked separately in Prometheus

## Task 2: Request Duration Histogram and Duplicate Mitigation

### Step 1: Querying Request Duration with Histogram

To analyze request duration, we use the `istio_request_duration_milliseconds_bucket` metric, focusing on the `frontend` to `shippingservice` service pair. This query should convert cumulative data to an incremental rate over the past 1m.

#### Prometheus Query for Histogram of Request Durations

```prometheus
sum by (le)(rate(istio_request_duration_milliseconds_bucket{
    source_canonical_service="frontend",
    destination_canonical_service="shippingservice"
}[1m]))
```

### Handling Duplicates in Histograms

The duplication issue here stems from:

- **Response flags** generated by Istio, especially during fault injection, labeled as `response_flag="DI"`
- Using `sum by (le)` merges these multiple histograms into a single set of values

## Task 3: Fault Injection on the `shippingservice`

### Step 1: Configuring the Initial Fault Injection Rule

We created a YAML file named `shippingservice-fault-injection.yaml` to set up fault injection. In this configuration:

- Requests from the `frontend` service had a 0.2-second delay
- Requests from the `checkoutservice` with URIs containing `GetQuote` had a 0.4-second delay

### Step 2: Applying the Fault Injection

Apply the fault injection rule with:

```bash
kubectl apply -f shippingservice-fault-injection.yaml
```

### Step 3: Observing Results in Grafana and Kiali

- **Grafana:** Set up the histogram for `istio_request_duration_milliseconds_bucket` using `sum by (le)` to visualize latency from fault injection
- **Kiali:** Verify that delays affect only the `frontend` and `checkoutservice` traffic by comparing the `Source` and `Destination` metrics

### Task 3 Screenshot

Check the `task3` folder for screenshots of Grafana and Kiali dashboards showing both peaks for delayed and non-delayed requests.

## Observing Changes with Multiple Pods

1. Scale the `loadgenerator` deployment:

```bash
kubectl scale deployment loadgenerator --replicas=2
```

2. Verify pod scaling with:

```bash
kubectl get pods -l app=loadgenerator
```

3. Monitor the real-time impact on Grafana by observing request rates and durations for scaled loads.

## Summary

This guide covers:

- Setting up Prometheus queries for request rates and request duration histograms
- Configuring Istio fault injection to simulate latency
- Handling and understanding duplicate entries in Prometheus queries
- Monitoring and visualizing metrics in Grafana and Kiali

For screenshots of each task, see the respective folders: `task1`, `task2`, and `task3`.
I'll help format the markdown with proper spacing, headers, and consistent styling.
